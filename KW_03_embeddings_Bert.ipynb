{"cells":[{"cell_type":"markdown","source":["#Section 1: Understand Bert Endcoding Tokenization and Output<br>\n"],"metadata":{"id":"VDocKqSTCUkD"}},{"cell_type":"markdown","source":["###Part1: Understand Bert Endcoding Tokenization<br>\n","comparing to approaches:\n","1. using tokenizer, with output including all info, input_ids, attention_mask, token_type_ids <br>\n","2. using tokenizer function to proceed each steps to reach the same tokenization results`"],"metadata":{"id":"CrzJ9QueJXhF"}},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertModel"],"metadata":{"id":"wLLVcs2_CN9O","executionInfo":{"status":"ok","timestamp":1713239563945,"user_tz":240,"elapsed":151,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["**1. using tokenizer, with output including all info, input_ids, attention_mask, token_type_ids**"],"metadata":{"id":"nEo9REMeDLfv"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tO8HXB4T1N3q","executionInfo":{"status":"ok","timestamp":1713239343244,"user_tz":240,"elapsed":1199,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"0bd1b0c0-b784-48f3-c1e8-3ef49edc4699"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[  101,  7592,  1010, 14324,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n","torch.Size([1, 6, 768])\n"]}],"source":["# Load pre-trained model and tokenizer\n","model = BertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize input\n","input_text = \"Hello, BERT!\"\n","inputs = tokenizer(input_text, return_tensors='pt')   # return_tensors=\"pt\" is to specify return pytorch type of tensors\n","print(inputs)\n","'''\n","===============================================\n","input_ids is the tokens_id\n","attention_mask is the mask to filter out '[PAD]' token\n","token_type_ids sepcifies to which segement the token belongs to, in multiple text segments tasks, such as question-answering\n","===============================================\n","'''\n","\n","# If you specifically want just the initial embeddings:\n","initial_embeddings = model.embeddings(inputs['input_ids'])     # use the input_id to map for the pretrained embedding representation\n","print(initial_embeddings.shape)\n"]},{"cell_type":"markdown","source":["***2. using tokenizer function to proceed each steps to reach the same tokenization results***"],"metadata":{"id":"Z8hEXsy6DS5T"}},{"cell_type":"code","source":["tokens = tokenizer.tokenize(input_text)\n","tokens = ['[CLS]'] + tokens + ['[SEP]']\n","tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n","mask = [1 if ele != '[PAD]' else 0 for ele in tokens]\n","print(tokens, tokens_id, mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaUnejtD97oT","executionInfo":{"status":"ok","timestamp":1713238841010,"user_tz":240,"elapsed":137,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"f7272778-2116-4c22-8ed7-8904e3e97f65"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'hello', ',', 'bert', '!', '[SEP]'] [101, 7592, 1010, 14324, 999, 102] [1, 1, 1, 1, 1, 1]\n"]}]},{"cell_type":"markdown","source":["###Part2: Understand Bert Endcoding Output<br>"],"metadata":{"id":"KjeWrYFoD2uB"}},{"cell_type":"code","source":["# This gives you the embeddings after all transformations.\n","with torch.no_grad():\n","    outputs = model(**inputs)      # torch.no_grad(): run the below codes without gradient descent.\n","    last_hidden_state = outputs.last_hidden_state\n","    pooler_output = outputs.pooler_output\n","'''\n","===============================================\n","'ast_hidden_state'\n","    It is the output of the last layer of the BERT model. It contains the contextualized embeddings for each token attending to other tokens in the input sequence.\n","    It is is typically used in tasks where token-level representations are required, such as sequence labeling (e.g., named entity recognition) or token-level classification tasks.\n","    dimention = [batch_size, token_size, embedding_size]\n","'pooler_output'\n","    It is a single vector representation of the entire input sequence, by applying a pooling operation over the last_hidden_state results.\n","    In BERT-family of models, pooling operation is processing through a linear layer and a tanh activation function (similar to the pool layer in CNN) on classification token.,\n","    The purpose of the pooler_output is to provide a fixed-size representation of the entire input sequence, capturing its overall semantic content.\n","    This representation is often used as input to downstream tasks such as sentence classification, where a single representation of the entire sentence is required.\n","    dimention = [batch_size, embedding_size]\n","===============================================\n","'''\n","print(last_hidden_state.shape, pooler_output.shape)\n","print(last_hidden_state, pooler_output)"],"metadata":{"id":"DDnXJ5gmBpei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmxDTJxm1N3w"},"source":["#Section 2: How the embedding similarity of word \"Bank\" from difference sentences proceed through layers <br>\n","compare 3 processes:<br>\n","1. inital embedding similarity\n","2. last_hidden_state embedding similarity\n","3. the embedding similarity of each layers"]},{"cell_type":"markdown","source":["**1. inital embedding similarity** <br>\n","As the initial embedding is just mapped from token ID, thus the same word embedding should be similar, even from different context"],"metadata":{"id":"1MvCTtbgNfK0"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"KRR-1TwW1N3z","outputId":"cba9e686-89a8-4024-d65b-d94c28b94a15","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713242860976,"user_tz":240,"elapsed":742,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between the initial embeddings: 0.9999998807907104\n"]}],"source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","model_name = \"bert-base-uncased\"\n","model = BertModel.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# Function to get the initial embedding of a word from a sentence\n","def get_initial_word_embedding(sentence, word):\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    word_id = tokenizer.convert_tokens_to_ids(word)\n","    word_position = inputs[\"input_ids\"][0].tolist().index(word_id\n","    # Extracting the initial embeddings\n","    initial_embeddings = model.embeddings(inputs[\"input_ids\"])\n","    return initial_embeddings[0][word_position].detach().numpy()\n","\n","# Compare initial embeddings for the word 'bank' in two different contexts\n","sentence1 = \"I sat by the river bank.\"\n","sentence2 = \"I deposited money in the bank.\"\n","\n","embedding1 = get_initial_word_embedding(sentence1, \"bank\")\n","embedding2 = get_initial_word_embedding(sentence2, \"bank\")\n","\n","# Calculate cosine similarity or any other metric to see the difference\n","# For simplicity, let's use dot product\n","similarity = torch.nn.functional.cosine_similarity(\n","    torch.tensor(embedding1).unsqueeze(0), torch.tensor(embedding2).unsqueeze(0)\n",")\n","\n","print(f\"Cosine similarity between the initial embeddings: {similarity.item()}\")\n"]},{"cell_type":"markdown","source":["**2. last_hidden_state embedding similarity** <br>\n","In the final layer, the same initial embeddings are enriched with contextual semetics through attention and forward layers, thus they should be different based on their context."],"metadata":{"id":"n_CLDDxJN4cc"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"-Dky_weK1N3y","outputId":"84297c15-cb64-4294-c8e0-d7b8508aafdb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713242774009,"user_tz":240,"elapsed":2329,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between the embeddings: 0.5257285833358765\n"]}],"source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","model_name = \"bert-base-uncased\"\n","model = BertModel.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# Function to get the embedding of a word from a sentence\n","def get_word_embedding(sentence, word):\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    word_id = tokenizer.convert_tokens_to_ids(word)\n","    word_position = inputs[\"input_ids\"][0].tolist().index(word_id)\n","    return outputs[\"last_hidden_state\"][0][word_position].detach().numpy()\n","\n","# Compare embeddings for the word 'bank' in two different contexts\n","sentence1 = \"I sat by the river bank.\"\n","sentence2 = \"I deposited money in the bank.\"\n","\n","embedding1 = get_word_embedding(sentence1, \"bank\")\n","embedding2 = get_word_embedding(sentence2, \"bank\")\n","\n","# Calculate cosine similarity or any other metric to see the difference\n","# For simplicity, let's use dot product\n","similarity = torch.nn.functional.cosine_similarity(\n","    torch.tensor(embedding1).unsqueeze(0), torch.tensor(embedding2).unsqueeze(0)\n",")\n","\n","print(f\"Cosine similarity between the embeddings: {similarity.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"4YcqdU8M1N30"},"source":["**3. the embedding similarity of each layers**"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"NHC669P61N30","outputId":"368e6d49-9d92-4a7d-82d0-be8085860440","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713243390119,"user_tz":240,"elapsed":1377,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 0 (initial embeddings) similarity: 0.9999998807907104\n","Layer 1 similarity: 0.7583112120628357\n","Layer 2 similarity: 0.6887193322181702\n","Layer 3 similarity: 0.6551119685173035\n","Layer 4 similarity: 0.5860087275505066\n","Layer 5 similarity: 0.5718531012535095\n","Layer 6 similarity: 0.5652671456336975\n","Layer 7 similarity: 0.5206233263015747\n","Layer 8 similarity: 0.4970075488090515\n","Layer 9 similarity: 0.49416643381118774\n","Layer 10 similarity: 0.5007321834564209\n","Layer 11 similarity: 0.5522933006286621\n","Layer 12 similarity: 0.5257285833358765\n"]}],"source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","model_name = \"bert-base-uncased\"\n","model = BertModel.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","# Tokenize the sentences\n","sentence1 = \"I sat by the river bank.\"\n","sentence2 = \"I deposited money in the bank.\"\n","\n","inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n","inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n","\n","word_id = tokenizer.convert_tokens_to_ids(\"bank\")\n","word_position1 = inputs1[\"input_ids\"][0].tolist().index(word_id)\n","word_position2 = inputs2[\"input_ids\"][0].tolist().index(word_id)\n","\n","# Get initial embeddings\n","initial_embeddings1 = model.embeddings(inputs1[\"input_ids\"])\n","initial_embeddings2 = model.embeddings(inputs2[\"input_ids\"])\n","\n","cosine_sim = torch.nn.functional.cosine_similarity(initial_embeddings1[0][word_position1].unsqueeze(0),\n","                                                   initial_embeddings2[0][word_position2].unsqueeze(0))\n","print(f\"Layer 0 (initial embeddings) similarity: {cosine_sim.item()}\")\n","\n","# Process both sentences through each BERT layer\n","hidden_states1 = [initial_embeddings1]\n","hidden_states2 = [initial_embeddings2]\n","\n","for i, layer in enumerate(model.encoder.layer):\n","    layer_output1 = layer(hidden_states1[-1], attention_mask=inputs1[\"attention_mask\"])       # layer() uses previous hidden_state as input\n","    hidden_states1.append(layer_output1[0])\n","\n","    layer_output2 = layer(hidden_states2[-1], attention_mask=inputs2[\"attention_mask\"])\n","    hidden_states2.append(layer_output2[0])\n","\n","    cosine_sim = torch.nn.functional.cosine_similarity(hidden_states1[-1][0][word_position1].unsqueeze(0),\n","                                                       hidden_states2[-1][0][word_position2].unsqueeze(0))\n","    print(f\"Layer {i + 1} similarity: {cosine_sim.item()}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Wk3BIxm-9AqK"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"myusb_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}