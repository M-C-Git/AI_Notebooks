{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install numpy\n","!pip install tensorflow\n","!pip install keras\n"],"metadata":{"id":"6wV11PF-tkTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5BeMc5Dtcqo"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Define a small corpus\n","corpus = [\n","    \"the cat sat on the mat\",\n","    \"the dog sat on the log\",\n","]\n","\n","# Tokenize the corpus to create a vocabulary and sequences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1        # word_index are non-zero index, +1 to inlcude padding value 0\n","\n","# Convert sentences to sequences of tokens\n","input_sequences = []\n","for line in corpus:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]         # starting from 2 words to all the words in the sentences, because it will truncate off the last word for predictor, and use the last truncated word as label.\n","        input_sequences.append(n_gram_sequence)\n","\n","# Pad sequences for having the same length\n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# Prepare predictors and labels\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]    # truncate off the last word from sequencesas for predictor, and use the last truncated word as label.\n","label = to_categorical(label, num_classes=total_words)\n","\n","# Build the neural network model\n","model = Sequential()\n","model.add(Embedding(input_dim=total_words, output_dim=10, input_length=max_sequence_len-1))\n","model.add(LSTM(100, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(predictors, label, epochs=100, verbose=2)\n"]},{"cell_type":"code","source":["# to understanding the embedding layout output\n","\n","from keras import backend as K\n","embedding_layer_output = K.function([model.layers[0].input],[model.layers[0].output])([predictors])[0]\n","print(embedding_layer_output[0],'\\n', embedding_layer_output[1])\n","'''\n","========================================================\n","Embedding is nothing but a learning layer with weights vectors representing each vocaburary word.\n","Once input is a vectore, then map each element value to the embedding index to find the corresponding weights vector. the embedding_layer_output is a vector of 1 higher dimension.\n","Once input is a scaler, then map the scaler value to the embedding index to find the corresponding weights vector. the embedding_layer_output is a 1 dimention vector, which is exactly the word embedding.\n","========================================================\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYUq6OJgolHH","executionInfo":{"status":"ok","timestamp":1713336304587,"user_tz":240,"elapsed":185,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"e38f5591-5748-4c1e-8409-e22c9a7a62f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [ 0.07174324  0.00684089 -0.0886955   0.04066803 -0.0558828   0.06841912\n","   0.11332682 -0.00641044 -0.12822239  0.04211319]] \n"," [[-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [-0.11833682 -0.10135332  0.11872561 -0.09491817  0.12412032 -0.06265501\n","  -0.09566948  0.12450426  0.10115235 -0.07511616]\n"," [ 0.07174324  0.00684089 -0.0886955   0.04066803 -0.0558828   0.06841912\n","   0.11332682 -0.00641044 -0.12822239  0.04211319]\n"," [ 0.04514093  0.07841094 -0.07238209  0.01869517 -0.11699299  0.10270458\n","   0.1307155  -0.08503766 -0.03440688  0.0876802 ]]\n"]}]},{"cell_type":"code","source":["# Retrieve and print the embeddings\n","embeddings = model.layers[0].get_weights()[0]\n","\n","# Print the word and its corresponding embedding\n","word_index = tokenizer.word_index\n","for word, i in word_index.items():\n","    print(f\"Word: {word}, Embedding: {embeddings[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgNX5YOuhCeV","executionInfo":{"status":"ok","timestamp":1713336944773,"user_tz":240,"elapsed":222,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"6d6cba0f-ab03-4546-ba75-cfd34ba65fd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word: the, Embedding: [ 0.07174324  0.00684089 -0.0886955   0.04066803 -0.0558828   0.06841912\n","  0.11332682 -0.00641044 -0.12822239  0.04211319]\n","Word: sat, Embedding: [ 0.09968039  0.03270097 -0.0893901   0.0577278  -0.11386181  0.11115944\n","  0.02218372 -0.10860738 -0.07029856  0.03653787]\n","Word: on, Embedding: [ 0.11307956  0.09688695 -0.03744842  0.06424715 -0.10792501  0.07347568\n","  0.11631583 -0.01768629 -0.10164903  0.10215276]\n","Word: cat, Embedding: [ 0.04514093  0.07841094 -0.07238209  0.01869517 -0.11699299  0.10270458\n","  0.1307155  -0.08503766 -0.03440688  0.0876802 ]\n","Word: mat, Embedding: [-0.01083907  0.03675335 -0.04226586 -0.03329039 -0.0020213   0.04262426\n","  0.0178417   0.02759006  0.00922717 -0.01770638]\n","Word: dog, Embedding: [ 0.11248654 -0.01926392 -0.08886297  0.01913746 -0.06291626  0.1295239\n","  0.11268586  0.03601092 -0.09354326  0.03238495]\n","Word: log, Embedding: [ 0.03709486  0.04440585 -0.02342044 -0.02243969 -0.02530271  0.04582239\n"," -0.04829375  0.04778634 -0.04259117 -0.01165665]\n"]}]},{"cell_type":"code","source":["def predict_next_word(model, tokenizer, text_sequence):\n","    \"\"\"\n","    Predict the next word based on the input text sequence.\n","\n","    Parameters:\n","    - model: The trained Keras model.\n","    - tokenizer: The tokenizer used to preprocess the text data.\n","    - text_sequence: A string containing the sequence of text.\n","\n","    Returns:\n","    - The predicted next word.\n","    \"\"\"\n","    # Convert the text sequence to a sequence of integers\n","    sequence = tokenizer.texts_to_sequences([text_sequence])[0]\n","    # Pad the sequence to match the input shape of the model\n","    padded_sequence = pad_sequences([sequence], maxlen=max_sequence_len-1, padding='pre')\n","    # Predict the next word (as a probability distribution over the vocabulary)\n","    predictions = model.predict(padded_sequence, verbose=0)\n","    # Convert the probabilities to an integer index\n","    predicted_index = np.argmax(predictions, axis=-1)[0]\n","    # Map the integer index to the corresponding word\n","    predicted_word = tokenizer.index_word[predicted_index]\n","\n","    return predicted_word\n","\n","# Example usage\n","# text_sequence = \"the cat sat on\"\n","text_sequence = \"cats and dogs are\"\n","predicted_word = predict_next_word(model, tokenizer, text_sequence)\n","print(f\"Given the sequence '{text_sequence}', the predicted next word is '{predicted_word}'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VIq3gItwL59","outputId":"be716f1e-959f-44ee-f980-e3b15514e216"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Given the sequence 'cats and dogs are', the predicted next word is 'friends'.\n"]}]},{"cell_type":"markdown","source":["**Summary**<br>\n","1. Embedding is nothing but a mapping matrix, which has one row per vocabulary word (think as index), and all other column values of this row as word embedding vector.<br>\n","To convert a word index to an embedding, the Embeding layer just looks up and return the row embedding vector that corresponds to that workd index.<br>\n","2. Embedding itself is a learning layer, with weights orginzed in a vector shape that presents word vector.<br>\n","3. If Embedding layer is in a specific task, then the embedding is customized to that task, the embedding result is highly likely without semantic meaning.<br>\n","If Embedding layer is in a context prediction task, such as this example or Word2Vec, then the embedding result is enrished with semantic meanings.\n","\n"],"metadata":{"id":"eFVC6LJt1-Et"}}]}