{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"358e03a0263e4a66b1aa420f45f23d0c":{"model_module":"@jupyter-widgets/controls","model_name":"FileUploadModel","model_module_version":"1.5.0","state":{"_counter":4,"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FileUploadModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FileUploadView","accept":"image/png","button_style":"","data":[null],"description":"Upload","description_tooltip":null,"disabled":false,"error":"","icon":"upload","layout":"IPY_MODEL_0d65c72d7d3e4b8fa6aee6dbf8baa093","metadata":[{"name":"fmnist_image_1.png","type":"image/png","size":560,"lastModified":1709300816381}],"multiple":false,"style":"IPY_MODEL_8aa89285e88147ba9cfb35e461e57e0b"}},"0d65c72d7d3e4b8fa6aee6dbf8baa093":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aa89285e88147ba9cfb35e461e57e0b":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"cells":[{"cell_type":"code","source":["  import tensorflow as tf\n","  from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n","  from tensorflow.keras.models import Sequential\n","  from tensorflow.keras.utils import to_categorical\n","  from tensorflow.keras.callbacks import EarlyStopping\n","  from sklearn.model_selection import train_test_split\n","\n","  # Load the dataset\n","  (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","  # Normalize the pixel values\n","  train_images = train_images / 255.0\n","  test_images = test_images / 255.0\n","\n","  # Reshape the images to include channel dimension (grayscale)\n","  train_images = train_images.reshape((-1, 28, 28, 1))\n","  test_images = test_images.reshape((-1, 28, 28, 1))\n","\n","  # Convert labels to one-hot encoding\n","  train_labels = to_categorical(train_labels)\n","  test_labels = to_categorical(test_labels)\n","\n","  # Split the training data into training and validation sets\n","  train_images, val_images, train_labels, val_labels = train_test_split(\n","      train_images, train_labels, test_size=0.2, random_state=42\n","  )\n","\n","  # Build the CNN model with dropout regularization\n","  model = Sequential([\n","      Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","      MaxPooling2D(2, 2),\n","      Conv2D(64, (3, 3), activation='relu'),\n","      MaxPooling2D(2, 2),\n","      Flatten(),\n","      Dense(128, activation='relu'),\n","      Dropout(0.5),  # Dropout layer for regularization\n","      Dense(10, activation='softmax')\n","  ])\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","  # Early stopping callback\n","  early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001, restore_best_weights=True)\n","\n","  # Train the model with early stopping\n","  model.fit(\n","      train_images, train_labels,\n","      batch_size=32,\n","      epochs=20,\n","      validation_data=(val_images, val_labels),\n","      callbacks=[early_stopping]\n","  )\n","\n","  # Evaluate the model on the test set\n","  test_loss, test_acc = model.evaluate(test_images, test_labels)\n","  print('Test accuracy:', test_acc)"],"metadata":{"id":"QggvQiH1q4_R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"639c7e20-7721-48a4-e445-8526b0393ece"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 2s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 1s 0us/step\n","Epoch 1/20\n","1500/1500 [==============================] - 11s 4ms/step - loss: 0.6008 - accuracy: 0.7800 - val_loss: 0.3804 - val_accuracy: 0.8566\n","Epoch 2/20\n","1500/1500 [==============================] - 5s 4ms/step - loss: 0.4020 - accuracy: 0.8544 - val_loss: 0.3196 - val_accuracy: 0.8804\n","Epoch 3/20\n","1500/1500 [==============================] - 6s 4ms/step - loss: 0.3478 - accuracy: 0.8752 - val_loss: 0.3088 - val_accuracy: 0.8841\n","Epoch 4/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.3149 - accuracy: 0.8852 - val_loss: 0.2839 - val_accuracy: 0.8936\n","Epoch 5/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2886 - accuracy: 0.8957 - val_loss: 0.2696 - val_accuracy: 0.9011\n","Epoch 6/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2678 - accuracy: 0.9007 - val_loss: 0.2593 - val_accuracy: 0.9057\n","Epoch 7/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2480 - accuracy: 0.9082 - val_loss: 0.2548 - val_accuracy: 0.9047\n","Epoch 8/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2346 - accuracy: 0.9135 - val_loss: 0.2529 - val_accuracy: 0.9076\n","Epoch 9/20\n","1500/1500 [==============================] - 5s 4ms/step - loss: 0.2230 - accuracy: 0.9174 - val_loss: 0.2507 - val_accuracy: 0.9120\n","Epoch 10/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2126 - accuracy: 0.9217 - val_loss: 0.2410 - val_accuracy: 0.9118\n","Epoch 11/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.2038 - accuracy: 0.9228 - val_loss: 0.2611 - val_accuracy: 0.9101\n","Epoch 12/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.1930 - accuracy: 0.9273 - val_loss: 0.2605 - val_accuracy: 0.9065\n","Epoch 13/20\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.1857 - accuracy: 0.9307 - val_loss: 0.2724 - val_accuracy: 0.9081\n","313/313 [==============================] - 1s 2ms/step - loss: 0.2535 - accuracy: 0.9097\n","Test accuracy: 0.9096999764442444\n"]}]},{"cell_type":"code","source":["print(f\"The epoch with the lowest validation loss is: {early_stopping.stopped_epoch - early_stopping.patience + 1}\")"],"metadata":{"id":"sA3CtsE7zdhp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a48b0b0-23f9-4270-d62f-67a5e16dc71e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The epoch with the lowest validation loss is: 10\n"]}]},{"cell_type":"code","source":["import ipywidgets as widgets\n","from IPython.display import display\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from io import BytesIO\n","import numpy as np\n","import tensorflow as tf\n","\n","# Load your trained model here\n","# model = ...\n","\n","# Assuming class_names is defined as before\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# Function to load and preprocess an image from BytesIO\n","def load_and_preprocess_image_from_bytes(img_bytes):\n","    img = load_img(img_bytes, color_mode='grayscale', target_size=(28, 28))\n","    img = img_to_array(img)\n","    img = img.reshape(1, 28, 28, 1)  # Add batch dimension\n","    img = img / 255.0  # Normalize the image\n","    return img\n","\n","# Function to handle file selection and make a prediction\n","def on_file_selected(change):\n","    if change['type'] == 'change' and change['name'] == 'value':\n","        uploaded_file = next(iter(change['new'].values()))  # Get the first uploaded file\n","        content = uploaded_file['content']\n","        img_bytes = BytesIO(content)\n","\n","        img = load_and_preprocess_image_from_bytes(img_bytes)\n","        predictions = model.predict(img)\n","        predicted_class = np.argmax(predictions, axis=1)[0]\n","\n","        print(\"Predicted class:\", class_names[predicted_class])\n","\n","# Create a file upload widget\n","file_picker = widgets.FileUpload(\n","    accept='image/png',  # Specify allowed file type\n","    multiple=False  # Allow single file selection only\n",")\n","file_picker.observe(on_file_selected, names='value')\n","\n","# Display the file picker widget\n","display(file_picker)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["358e03a0263e4a66b1aa420f45f23d0c","0d65c72d7d3e4b8fa6aee6dbf8baa093","8aa89285e88147ba9cfb35e461e57e0b"]},"id":"Hj0_ZL2DRsnj","outputId":"c0a0b15a-8c77-4780-d97f-569475c5ac0b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["FileUpload(value={}, accept='image/png', description='Upload')"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358e03a0263e4a66b1aa420f45f23d0c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 226ms/step\n","Predicted class: Ankle boot\n","1/1 [==============================] - 0s 21ms/step\n","Predicted class: Dress\n","1/1 [==============================] - 0s 24ms/step\n","Predicted class: Dress\n","1/1 [==============================] - 0s 24ms/step\n","Predicted class: T-shirt/top\n"]}]}]}