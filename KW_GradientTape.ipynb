{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPA/dKQEnBWAb2jjQbWaAxE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Xy3QjtrRv3yu","executionInfo":{"status":"ok","timestamp":1714946087445,"user_tz":240,"elapsed":8442,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["x = tf.Variable(2.0)\n","with tf.GradientTape() as tape:\n","  y = 3 * x ** 4                # define a funciton with variable x\n","  dy_dx = tape.gradient(y, x)   # use GrandientTape to calcualte derivative for the function w.r.t variable x\n","print(dy_dx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G54GIiKbv-xf","executionInfo":{"status":"ok","timestamp":1714946189856,"user_tz":240,"elapsed":294,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"1396e86c-1b6b-4f3e-9fa6-a1ce276834a0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(96.0, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# generate some synthetic data for domonstration\n","x_train = tf.constant([1.0, 2.0, 3.0, 4.0], dtype=tf.float32)\n","y_true = tf.constant([2.0, 4.0, 6.0, 8.0], dtype=tf.float32)\n","\n","#Initialize the weights = 1\n","w = tf.Variable(1.0, dtype=tf.float32)\n","\n","# Define the linear regression model without bias\n","def linear(x):\n","  return w * x\n","\n","# Define the mean squared error loss\n","def mse(y_true, y_pred):\n","  return tf.reduce_mean(tf.square(y_true - y_pred))\n","\n","# training loop using tf.GradientTape\n","learning_rate = 0.01\n","epochs = 100\n","\n","for epoch in range(epochs):\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    y_pred = linear(x_train)\n","\n","    #Calculate mean square loss\n","    loss = mse(y_true, y_pred)\n","\n","  # compute gradient with respect to weight w\n","  gradient = tape.gradient(loss, w)\n","\n","  #update weight using gradient descent\n","  w.assign_sub(learning_rate * gradient)\n","\n","  # Pint the loss for every 10 epochs\n","  if epoch % 10 == 0:\n","    print(f\"Epoch {epoch}: Loss {loss.numpy()}, Gradient {gradient.numpy()}, Weight {w.numpy()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCSHzJtj_Aos","executionInfo":{"status":"ok","timestamp":1714967536441,"user_tz":240,"elapsed":330,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}},"outputId":"cfa6a01b-c5df-4c2e-d3c4-d4bc9abf2396"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss 7.5, Gradient -15.0, Weight 1.149999976158142\n","Epoch 10: Loss 0.2906963527202606, Gradient -2.953115463256836, Weight 1.832656741142273\n","Epoch 20: Loss 0.011267285794019699, Gradient -0.5813936591148376, Weight 1.9670543670654297\n","Epoch 30: Loss 0.00043670617742463946, Gradient -0.11446040868759155, Weight 1.9935139417648315\n","Epoch 40: Loss 1.692865407676436e-05, Gradient -0.022535741329193115, Weight 1.9987229108810425\n","Epoch 50: Loss 6.560999281646218e-07, Gradient -0.00443655252456665, Weight 1.9997485876083374\n","Epoch 60: Loss 2.5496280642300917e-08, Gradient -0.0008745789527893066, Weight 1.9999504089355469\n","Epoch 70: Loss 9.822542779147625e-10, Gradient -0.000171661376953125, Weight 1.9999902248382568\n","Epoch 80: Loss 3.807443249570497e-11, Gradient -3.3795833587646484e-05, Weight 1.9999980926513672\n","Epoch 90: Loss 8.988365607365267e-13, Gradient -5.185604095458984e-06, Weight 1.9999996423721313\n"]}]}]}