{"cells":[{"cell_type":"markdown","source":["# General steps (simplified):\n","1. prepare data\n","2. build model\n","3. define loss function, and optimizer\n","4. do epoch iteration:\n","    1. forward processing: call model_defined() predict y value\n","    2. backward processing: call loss.backward() to populate gradients\n","    3. call optimizer.step() to update weights\n","    4. call optimizer.zero_grad() to clear gradients, and prepare for next epoch iteration."],"metadata":{"id":"yeyuRG6Kblet"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"byen7B0TJPYJ","executionInfo":{"status":"ok","timestamp":1714320738408,"user_tz":240,"elapsed":106,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[],"source":["import torch\n","batch, dim_in, dim_h, dim_out = 128, 2000, 200, 20"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"VQ9oJSzxJU1L","executionInfo":{"status":"ok","timestamp":1714320740892,"user_tz":240,"elapsed":154,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[],"source":["# pepare data\n","input_X = torch.randn(batch, dim_in)\n","output_Y = torch.randn(batch, dim_out)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Cd5TVZ17JU4W","executionInfo":{"status":"ok","timestamp":1714320742236,"user_tz":240,"elapsed":96,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[],"source":["# Build a model\n","Adam_model = torch.nn.Sequential(\n","    torch.nn.Linear(dim_in, dim_h),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(dim_h, dim_out),)"]},{"cell_type":"code","source":["# define loss function\n","loss_fn = torch.nn.MSELoss(reduction='sum')"],"metadata":{"id":"JevMqLAaZzOH","executionInfo":{"status":"ok","timestamp":1714320743705,"user_tz":240,"elapsed":121,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"OqRmJLlHJU7C","executionInfo":{"status":"ok","timestamp":1714320744912,"user_tz":240,"elapsed":101,"user":{"displayName":"Min Chen","userId":"17937423999888765307"}}},"outputs":[],"source":["# define optimizer\n","rate_learning = 1e-4\n","optimizer = torch.optim.Adam(Adam_model.parameters(), lr=rate_learning)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3931,"status":"ok","timestamp":1714320750085,"user":{"displayName":"Min Chen","userId":"17937423999888765307"},"user_tz":240},"id":"PF5ctp-UJU9n","outputId":"4e30a48d-6ee5-4ffe-c794-32bbb5a08d17"},"outputs":[{"output_type":"stream","name":"stdout","text":["9 1963.87060546875\n","109 21.832597732543945\n","209 0.00417353305965662\n","309 1.0652828450474772e-06\n","409 4.0545145019166284e-10\n"]}],"source":["# do epoch iteration, each with following steps:\n","#    1. forward processing: predict y value\n","#    2. backward processing: call loss.backward() to populate gradients\n","#    3. call optimizer.step() to update weights\n","#    4. call optimizer.zero_grad() to clear gradients, and prepare for next epoch iteration.\n","\n","for epoch in range(500):\n","\n","  pred_y = Adam_model(input_X)\n","\n","  loss = loss_fn(pred_y, output_Y)\n","\n","  loss.backward()\n","  # Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter.\n","  # computes the gradients w.r.t. the parameters (those included in optimizer) using backpropagation.\n","  optimizer.step()\n","  # call optimizer one time to update weights based on the gradients of the parameters. 2) should not modify the .grad field of the parameters. -->optimizer.step()\n","  # 1) optimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule (Performs a single optimization step); 2) should not modify the .grad field of the parameters.\n","  optimizer.zero_grad()\n","  # clears existing gradients from the last step (otherwise youâ€™d just accumulate the next computed gradients from all loss.backward() calls)\n","  # clears from optimizer gradient group, not directly clear model gradient\n","\n","\n","  if epoch % 100 == 9:\n","    print(epoch, loss.item())  # calculate based on the model weights"]},{"cell_type":"markdown","source":["# Miscellaneous"],"metadata":{"id":"lbBWIvGge73t"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1714319928394,"user":{"displayName":"Min Chen","userId":"17937423999888765307"},"user_tz":240},"id":"9cvni779fp8X","outputId":"96a34bdb-9a8e-458b-d2a2-f48f2db7844e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ -4.,  -8., -12.]) 3.0\n","tensor([ -8., -16., -24.]) 3.0\n","tensor([-12., -24., -36.]) 3.0\n","tensor([-16., -32., -48.]) 3.0\n","tensor([-20., -40., -60.]) 3.0\n"]}],"source":["# this code demonstrates how loss function works:\n","# Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter.\n","\n","import torch\n","\n","# Create input tensor\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","output_Y = torch.tensor([2.0, 5.0, 10.0], requires_grad=True)\n","loss_func = torch.nn.MSELoss(reduction='sum')\n","for i in range(5):\n","  # Perform some operations\n","  y = x ** 2\n","  loss = loss_func(y, output_Y)   # by using MSELoss, x**2 --> x**4\n","  optimizer = torch.optim.Adam(Adam_model.parameters(), lr=rate_learning)\n","\n","  # Compute gradients with respect to x\n","  loss.backward()\n","\n","  # Gradients are computed and stored in x.grad\n","  print(x.grad, loss.item())"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHXhK5AMaGrZIgDuyzgBMq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}